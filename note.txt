
in rest-service, prometheus:
    - use same port 5000 instead of a separated port since the traffic in a lab is low and reduce comlplexity and also possible because they both use plain HTTP 
    - scrape
        + http_request_duration_seconds by [methods, endpoint]
        + http_requests_total by [method, endpoint, status_code]     

    - golden signals:
        + rate last 1 minute: rate(http_requests_total[1m]) 
        + latency 95%: histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))
        + error rate: sum(rate(http_requests_total{status_code=~"5..|4.."}[5m])) / sum(rate(http_requests_total[5m]))


in grpc-service, 
    - prometheus use different port - 9103 since gRPC using HTTP/2 while Prometheus use HTTP
    - prometheus works as interceptor
    - scrape:
        + grpc_server_handling_seconds with custom bucket 0.5ms since very fast
        
    - golden signals:
        + latency 95%: histogram_quantile(0.95, sum(rate(grpc_server_handling_seconds_bucket[5m])) by (le))



simulate the traffic by creating a python script, sending request every 2 secs

prometheus screenshot explain

[rest-service & gRPC-service]
              └─(OTLP-gRPC) → [otel-collector:4317 (manually configured)] 
                                              └─(OTLP-gRPC) → [jaeger:4317 (internally supported by framework)]

not sure why rest-service response consists of 3 small spans, maybe the framework design?

0.5ms increase in latency  due to OpenTelemetry instrumentation
    + this is 10% of timing in this setup -> worth it for important calls or calls that is not ultra low latency
    solution could be only trace a percentage of requests instead of everything (supported also by otel)
    
there are conflicts between grpc & opentelemetry libraries that leads to version downgrade and rebuilt since binary not available


portainer: admin, H.g.t.124101                           